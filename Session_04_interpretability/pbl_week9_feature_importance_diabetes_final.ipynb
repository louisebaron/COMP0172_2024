{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "g9A2ZxHTSO2s",
        "KmDf2DPDSO21"
      ]
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Lab session (PBL Week #9):**\n",
        "\n",
        "This week we are going to look at the different methods to compute feature importance, and highilight some of their downsides.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Dataset :**\n",
        "\n",
        "The dataset is part of the large dataset held at the National Institutes of Diabetes-Digestive-Kidney Diseases (NIH). The target variable is specified as \"Outcome\"; 1 indicates positive diabetes test result, 0 indicates negative.\n",
        "\n",
        "### **Variables :**\n",
        "* Pregnancies    : Number of pregnancies\n",
        "* Glucose        : 2-hour plasma glucose concentration in the oral glucose tolerance test\n",
        "* Blood Pressure : Blood Pressure (Smallness) (mm Hg)\n",
        "* SkinThickness  : Skin Thickness\n",
        "* Insulin        : 2-hour serum insulin (mu U/ml)\n",
        "* Diabetes Pedigree Function : Function (2 hour plasma glucose concentration in oral glucose tolerance test)\n",
        "* BMI            : Body mass index\n",
        "* Age            : Age (years)\n",
        "* Outcome        : Have the disease(1) or not (0)\n",
        "\n",
        "\n",
        "Read more: https://www.sciencedirect.com/science/article/pii/S2352914819300176"
      ],
      "metadata": {
        "id": "g8ppk0s6SO2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”Ž 1. Import Libraries, preprocessing, etc (Nothing to do)"
      ],
      "metadata": {
        "id": "g9A2ZxHTSO2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "# !pip install missingno\n",
        "import missingno as msno\n",
        "from datetime import date\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n",
        "\n",
        "# some adjustments\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "pd.set_option('display.width', 500)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "df_ = pd.read_csv('diabetes.csv')\n",
        "df = df_.copy()\n",
        "\n",
        "# auxiliary functions\n",
        "def check_df(dataframe):\n",
        "    print(\"##################### Shape #####################\")\n",
        "    print(dataframe.shape)\n",
        "    print(\"##################### Types #####################\")\n",
        "    print(dataframe.dtypes)\n",
        "    print(\"##################### Head #####################\")\n",
        "    print(dataframe.head(3))\n",
        "    print(\"##################### Tail #####################\")\n",
        "    print(dataframe.tail(3))\n",
        "    print(\"##################### NA #####################\")\n",
        "    print(dataframe.isnull().sum())\n",
        "    print(\"##################### Quantiles #####################\")\n",
        "    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n",
        "\n",
        "\n",
        "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ------\n",
        "        dataframe: dataframe\n",
        "\n",
        "        cat_th: int, optional\n",
        "\n",
        "        car_th: int, optinal\n",
        "\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "        cat_cols: list\n",
        "                Categorical features\n",
        "        num_cols: list\n",
        "                Numerical features\n",
        "        cat_but_car: list\n",
        "               Categorical view cardinal variable list\n",
        "\n",
        "    Examples\n",
        "    ------\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"iris\")\n",
        "        print(grab_col_names(df))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # cat_cols, cat_but_car\n",
        "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
        "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
        "                   dataframe[col].dtypes != \"O\"]\n",
        "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
        "                   dataframe[col].dtypes == \"O\"]\n",
        "    cat_cols = cat_cols + num_but_cat\n",
        "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
        "\n",
        "    # num_cols\n",
        "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
        "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
        "\n",
        "    print(f\"Observations: {dataframe.shape[0]}\")\n",
        "    print(f\"Variables: {dataframe.shape[1]}\")\n",
        "    print(f'cat_cols: {len(cat_cols)}')\n",
        "    print(f'num_cols: {len(num_cols)}')\n",
        "    print(f'cat_but_car: {len(cat_but_car)}')\n",
        "    print(f'num_but_cat: {len(num_but_cat)}')\n",
        "    return cat_cols, num_cols, cat_but_car\n",
        "\n",
        "def missing_values_table(dataframe, na_name=False):\n",
        "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
        "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
        "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
        "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
        "    print(missing_df, end=\"\\n\")\n",
        "    if na_name:\n",
        "        return na_columns\n",
        "\n",
        "def missing_vs_target(dataframe, target, na_columns):\n",
        "    temp_df = dataframe.copy()\n",
        "\n",
        "    for col in na_columns:\n",
        "        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n",
        "\n",
        "    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n",
        "\n",
        "    for col in na_flags:\n",
        "        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n",
        "                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n",
        "\n",
        "df.columns = [col.upper() for col in df.columns]\n",
        "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:46.523762Z",
          "iopub.execute_input": "2022-01-17T13:22:46.52426Z",
          "iopub.status.idle": "2022-01-17T13:22:46.558145Z",
          "shell.execute_reply.started": "2022-01-17T13:22:46.524143Z",
          "shell.execute_reply": "2022-01-17T13:22:46.55745Z"
        },
        "trusted": true,
        "id": "U5MQknD0SO2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”Ž 2. Missing Value Inputation (Nothing to do)\n",
        "\n",
        "At a first glance there are no missing values. However, if you look at BLOODPRESSURE (or BMI or INSULIN for example) you can notice some values which do not make any sense.  \n",
        "\n"
      ],
      "metadata": {
        "id": "KmDf2DPDSO21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n",
        "#df=df.drop([\"STDS: TIME SINCE LAST DIAGNOSIS\", \"STDS: TIME SINCE FIRST DIAGNOSIS\", \"HINSELMANN\", \"SCHILLER\", \"CITOLOGY\"], axis=1)\n",
        "df[[\"GLUCOSE\",\"BLOODPRESSURE\",\"SKINTHICKNESS\",\"INSULIN\",\"BMI\"]]= df[[\"GLUCOSE\",\"BLOODPRESSURE\",\"SKINTHICKNESS\",\"INSULIN\",\"BMI\"]].replace(0,np.NaN)\n",
        "na_cols = missing_values_table(df, True)\n",
        "\n",
        "#df.rename(columns={\"BIOPSY\": \"OUTCOME\"}, inplace=True)\n",
        "#df = df.reindex(sorted(df.columns), axis=1)\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:50.984981Z",
          "iopub.execute_input": "2022-01-17T13:22:50.985636Z",
          "iopub.status.idle": "2022-01-17T13:22:50.994Z",
          "shell.execute_reply.started": "2022-01-17T13:22:50.985604Z",
          "shell.execute_reply": "2022-01-17T13:22:50.993053Z"
        },
        "trusted": true,
        "id": "kYsUTP37SO21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace missing data with median values (Nothing to do)"
      ],
      "metadata": {
        "id": "u-6d0Jb0Ymff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def median_target(variable):\n",
        "    temp = df[df[variable].notnull()]\n",
        "    temp = temp[[variable, 'OUTCOME']].groupby(['OUTCOME'])[[variable]].median().reset_index()\n",
        "    return temp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:51.740603Z",
          "iopub.execute_input": "2022-01-17T13:22:51.740854Z",
          "iopub.status.idle": "2022-01-17T13:22:51.746751Z",
          "shell.execute_reply.started": "2022-01-17T13:22:51.740823Z",
          "shell.execute_reply": "2022-01-17T13:22:51.745904Z"
        },
        "trusted": true,
        "id": "e8HAmMdMSO23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = df.columns\n",
        "columns = columns.drop(\"OUTCOME\")\n",
        "\n",
        "for col in columns:\n",
        "    df.loc[(df['OUTCOME'] == 0) & (df[col].isnull()), col] = median_target(col)[col][0]\n",
        "    df.loc[(df['OUTCOME'] == 1) & (df[col].isnull()), col] = median_target(col)[col][1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:51.748476Z",
          "iopub.execute_input": "2022-01-17T13:22:51.749086Z",
          "iopub.status.idle": "2022-01-17T13:22:51.837317Z",
          "shell.execute_reply.started": "2022-01-17T13:22:51.749041Z",
          "shell.execute_reply": "2022-01-17T13:22:51.836165Z"
        },
        "trusted": true,
        "id": "nwS6-EgtSO23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df[cat_cols + num_cols], drop_first=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:52.14788Z",
          "iopub.execute_input": "2022-01-17T13:22:52.148259Z",
          "iopub.status.idle": "2022-01-17T13:22:52.164988Z",
          "shell.execute_reply.started": "2022-01-17T13:22:52.148221Z",
          "shell.execute_reply": "2022-01-17T13:22:52.16406Z"
        },
        "trusted": true,
        "id": "NEqFG7uRSO25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”Ž 3. Feature importance"
      ],
      "metadata": {
        "id": "Yu2oTQ4uSO26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ¦ 3.1 Logistic Regression\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n"
      ],
      "metadata": {
        "id": "_isjxhM7P9Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "y = df[\"OUTCOME\"]\n",
        "X=df.drop([\"OUTCOME\"], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
        "\n",
        "#Model 1\n",
        "lr_model=LogisticRegression().fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_test)\n",
        "print(\"accuracy: \", accuracy_score(y_pred, y_test))\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1-score:\", f1_score(y_test,y_pred))\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "probs = lr_model.predict_proba(X_test)\n",
        "y_pred_proba = probs[:,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "axes[0].plot(fpr,tpr,label=\"Logistic Regression, auc=\"+str(auc)[:5])\n",
        "axes[0].legend(loc=4)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cf_matrix, annot=True, ax=axes[1])\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:52.199068Z",
          "iopub.execute_input": "2022-01-17T13:22:52.199326Z",
          "iopub.status.idle": "2022-01-17T13:22:52.503211Z",
          "shell.execute_reply.started": "2022-01-17T13:22:52.199294Z",
          "shell.execute_reply": "2022-01-17T13:22:52.502129Z"
        },
        "trusted": true,
        "id": "FUEMCXmDSO26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â—Plot the coefficients of the logistic regression model. Use the function bellow or create your own. (*See* https://pandas.pydata.org/docs/reference/api/pandas.Series.plot.barh.html for an alternative.)\n",
        "\n",
        "\n",
        "â—Which is the most predictive feature according to the linear model?  "
      ],
      "metadata": {
        "id": "jhxgsEr10NNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_importance(model, feature_names, num=len(X)):\n",
        "    feature_imp = pd.DataFrame({'Value': model.coef_[0], 'Feature': feature_names})\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.set(font_scale=1)\n",
        "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n",
        "                                                                      ascending=False)[0:num])\n",
        "    plt.title('Features Coeff')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.tight_layout()\n",
        "\n",
        "##Write your code here\n",
        "## .......\n",
        "\n"
      ],
      "metadata": {
        "id": "lX9ONEQf0PcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â— What happens if we standardize the features for the Linear model?"
      ],
      "metadata": {
        "id": "OqJuyMD5MAF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rs = StandardScaler()\n",
        "X_std=X.copy()\n",
        "X_std[num_cols] = rs.fit_transform(X_std[num_cols])\n",
        "X_std.head()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.30, random_state=17)\n",
        "\n",
        "#Linear Model\n",
        "lr_model=LogisticRegression().fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_test)\n",
        "print(\"accuracy: \", accuracy_score(y_pred, y_test))\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1-score:\", f1_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "F4eCTzcOLwHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the coefficients\n",
        "## Write your code here. Use the attributes of the lr_model like in the previous cell\n",
        "#linear_importances = pd.Series(????, index=X_train.columns).sort_values(ascending=True)\n",
        "fig, ax = plt.subplots()\n",
        "linear_importances.plot.barh(ax=ax, color='purple')\n",
        "ax.set_title(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tLW5rZNJMIyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸŒ´ 3.2 Decision Trees\n",
        "\n",
        "Train and evaluate a Random Forrest model here (Nothing to do)"
      ],
      "metadata": {
        "id": "BhXUZHla9BcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier().fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "probs = rf_model.predict_proba(X_test)\n",
        "y_pred_proba = probs[:,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "axes[0].plot(fpr,tpr,label=\"Random Forrest, auc=\"+str(auc)[:5])\n",
        "axes[0].legend(loc=4)\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cf_matrix, annot=True, ax=axes[1])\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FIwb40MJEd7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Decrease in Impurity (MDI or Gini Importance)Â approximates each feature importance as the sum over the number of splits (across all tress) that include the feature, proportionally to the number of samples it splits.\n",
        "\n",
        "â—Measure the MDI of the trained RF model.\n",
        "\n",
        "*Hint:* Use directly **feature_importances_** attribute from the RF model: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zQPjcplu3Mzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "#First let's plot a tree from the RF\n",
        "\n",
        "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "_ = tree.plot_tree(rf_model.estimators_[5],\n",
        "                   feature_names=X_train.columns,\n",
        "                   class_names=[\"Diabetes\", 'Normal'],\n",
        "                   filled=True)\n",
        "\n",
        "\n",
        "# Put the manes of the features in a list\n",
        "feature_names = X_train.columns\n",
        "\n",
        "## Complete code here. Get the importances from the rf_model\n",
        "## importances = ....\n",
        "\n",
        "#Create a pandas series with the MDI importances and sort them\n",
        "forest_importances = pd.Series(importances, index=feature_names).sort_values(ascending=True)\n",
        "\n",
        "#Plot the importances\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.barh(ax=ax, color='purple')\n",
        "ax.set_title(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8VSRsBc93OCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ² **Random features**  \n",
        "\n",
        "The impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset. MDI feature importance are equally susceptible to high cardinal features. They can aslo inflate the importance of numerical features.\n",
        "https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html\n",
        "\n",
        "â— Add a feature with random real values (high cardinality)  \n",
        "\n",
        "â— Add a feature with random integer values (3 values, low cardinality)\n",
        "\n",
        "*Hint:* use the available methods in numpy:\n",
        "https://numpy.org/doc/1.14/reference/generated/numpy.random.RandomState.html\n",
        "\n",
        "â— Retrain and evaluate the model.\n"
      ],
      "metadata": {
        "id": "9l5Cb1ZlS1US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the X dataset\n",
        "X_modified = X.copy()\n",
        "\n",
        "#Add random number features using np.random\n",
        "rng = np.random.RandomState()\n",
        "\n",
        "## Write your code here. Use RNG to generate a random feature (similar to below))\n",
        "# X_modified[... = ...\n",
        "\n",
        "##  Use RNG to generate a random integer feature\n",
        "X_modified[\"RANDOM_CAT\"] = rng.randint(3, size=X.shape[0])\n",
        "\n",
        "\n",
        "# Split the set again\n",
        "X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(X_modified, y, test_size=0.30, random_state=23)\n",
        "#Fit a classifier\n",
        "rf_model_rand = RandomForestClassifier().fit(X_train_rand, y_train_rand)\n",
        "\n",
        "\n",
        "y_pred_rand = rf_model_rand.predict(X_test_rand)\n",
        "accuracy_score(y_pred_rand, y_test)\n",
        "#Plot the accuracy\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "probs = rf_model_rand.predict_proba(X_test_rand)\n",
        "y_pred_proba = probs[:,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test_rand,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test_rand, y_pred_proba)\n",
        "axes[0].plot(fpr,tpr,label=\"Random Forrest, auc=\"+str(auc)[:5])\n",
        "axes[0].legend(loc=4)\n",
        "cf_matrix = confusion_matrix(y_test_rand, y_pred_rand)\n",
        "sns.heatmap(cf_matrix, annot=True, ax=axes[1])\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "100W3MFS47LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â— Plot the MDI feature importances. What do you notice?"
      ],
      "metadata": {
        "id": "7Q_hiNnzWi9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train_rand.columns\n",
        "mdi_importances = pd.Series(\n",
        "    rf_model_rand.feature_importances_, index=feature_names\n",
        ").sort_values(ascending=True)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "## Write your code here ...\n",
        "### .....\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OqQIviNuWkx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸª“ 3.3 Drop features\n",
        "\n",
        "**â—Compute feature importance by leaving out the features one by one**\n",
        "\n",
        "Plot the distribution of some of the attributes side by side of the 2 training datasets.\n",
        "\n",
        "**â—Which feature seems to be the most important?**"
      ],
      "metadata": {
        "id": "HZSk4wAtUcLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the set again and fit the model (to remove the previous random features)\n",
        "#X=X.drop([\"RANDOM_CAT\", \"RANDOM_NUM\"], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=14)\n",
        "rf_model = RandomForestClassifier().fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "BQKbhQ1w23Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â— Complete the function that implements the feature importance based on dropping features from the dataset."
      ],
      "metadata": {
        "id": "N8FVaYaVqPoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_features(features,model=RandomForestClassifier(random_state=78), X_train=X_train, y_train=y_train,\n",
        "                  X_test=X_test, y_test=y_test):\n",
        "\n",
        "    # Train the model with all features first\n",
        "    initial_model = model.fit(X_train, y_train)\n",
        "    y_pred = initial_model.predict(X_test)\n",
        "    # compute its accuracy on the test set\n",
        "    initial_accuracy = accuracy_score(y_pred, y_test)\n",
        "    drop_importances = [] # a list which will contain the drop feature importances\n",
        "    # Remove each feature from the dataset one by one\n",
        "    for feature in features:\n",
        "      # Remove each feature from the train dataset and test dataset one by one\n",
        "      # !!! Write your code here:\n",
        "      ## X_train_drop = ??\n",
        "      ## X_test_drop = ??\n",
        "      # train a new model without the feature dropped\n",
        "      rf_model = model.fit(X_train_drop, y_train)\n",
        "\n",
        "      #compute the model's accuracy on the test set\n",
        "      y_pred_drop = rf_model.predict(X_test_drop)\n",
        "      drop_accuracy = accuracy_score(y_pred_drop, y_test)\n",
        "      #compute th difference in the accuracy with respect to the initial model (initial_accuracy)\n",
        "      # !!! Write your code here ...\n",
        "      # .. drop_importance = ???\n",
        "      # add the difference to the list\n",
        "      drop_importances.append(drop_importance)\n",
        "\n",
        "    return pd.Series(drop_importances, index=features).sort_values(ascending=True)\n",
        "\n",
        "drop_imp = drop_features(X_train.columns)\n",
        "ax =drop_imp.plot.barh(color='green')\n",
        "ax.set_title(\"Drop columns importance\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7XF68eTN3uXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸŽ² 3.4 Permutation importance\n",
        "\n",
        "Permutation feature importance measures the decrease in the model accuracy of the model after we permute the featureâ€™s values, breaking the relationship between the feature and the outcome.\n",
        "\n",
        "*Read more:*\n",
        "\n",
        "https://christophm.github.io/interpretable-ml-book/feature-importance.html\n",
        "\n",
        "\n",
        "â— Compute the permutation feature importance on the test set. *Hint*: use the available implementation ind sklearn:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html\n",
        "\n",
        "â—Which feature is the most predictive according to this metric?"
      ],
      "metadata": {
        "id": "j5AWQ94U_ILi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "#rf_model = RandomForestClassifier().fit(X_train, y_train)\n",
        "feature_names = X_train.columns\n",
        "\n",
        "## Compute the PI on the test set using the sklearn method. !!!! Write your code here\n",
        "## result = ....\n",
        "\n",
        "\n",
        "importances = pd.DataFrame(\n",
        "    result.importances.T,\n",
        "    columns=feature_names,\n",
        ")\n",
        "\n",
        "ax = importances.plot.box(vert=False, whis=10)\n",
        "ax.set_title(\"Permutation Importances (test set)\")\n",
        "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
        "ax.set_xlabel(\"Decrease in accuracy score\")\n",
        "ax.figure.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VwYuIfno6wUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ‘¯ 3.5 Correlated features\n",
        "\n",
        "Highly correlated features can decrease the  importance of these features, when dropping features one by one. Here, we simulate artificial correlated features, by simply duplicating one of the important features (columns) in the dataset.\n",
        "\n",
        "â— Use the drop feature approach on this new dataset. How does the importance change as compared to the initial set-up?"
      ],
      "metadata": {
        "id": "Vp-oh5c5Cewx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_duplicate=X.copy()\n",
        "\n",
        "\n",
        "## Write your code here\n",
        "## Add a 'INSULIN_copy' feature to the X_duplicate dataset\n",
        "## ...\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_duplicate, y, test_size=0.30, random_state=41)\n",
        "print(X_train.columns)\n",
        "\n",
        "\n",
        "## Write your code here...Call the drop_features function as before\n",
        "## drop_imp = ....\n",
        "\n",
        "ax =drop_imp.plot.barh(color='darkorange')\n",
        "ax.set_title(\"Drop columns importance\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RYOZ7l0I_L_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlated features can decrease the permutation importance of these features by splitting the importance between both features. Equally,  permutation feature importance can be biased by unrealistic data instances. *Read more:*\n",
        "\n",
        "https://christophm.github.io/interpretable-ml-book/feature-importance.html#disadvantages-9\n",
        "\n",
        "â—Compute the permutation importance for the new dataset that contains an identical copy of one the improtant features. How does the importance change with respect to the initial set up?"
      ],
      "metadata": {
        "id": "YCfeXVcYD_Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## We fit a RF classifier on this new dataset\n",
        "rf_model_perm = RandomForestClassifier().fit(X_train, y_train)\n",
        "### Write your code here ... (similar to 3.4)\n"
      ],
      "metadata": {
        "id": "CbwSG4j-EA1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â—[OPTIONAL] Try out other models, SVM, MLP, etc..."
      ],
      "metadata": {
        "id": "-krIX_fGYFE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ‹ 4. LIME:\n",
        "\n",
        "Local interpretable model-agnostic explanations (LIME) focuses on training local surrogate models to explain individual predictions.\n",
        "\n",
        "*Read more:*\n",
        "\n",
        "https://christophm.github.io/interpretable-ml-book/lime.html\n",
        "\n"
      ],
      "metadata": {
        "id": "2F5L1pcuHe2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install lime\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values.tolist(),\n",
        "                                                  verbose=True, mode='classification')"
      ],
      "metadata": {
        "id": "O--GfFgYHgGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â— Chose an instance from the test set (X_test) and explain the random forest's prediction.\n",
        "\n",
        "Check out tutorials on using LIME:\n",
        "https://github.com/marcotcr/lime\n",
        "\n",
        "\n",
        "Check out the explain_instance method from the lime documentation: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QeQpD64DDTdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = explainer.explain_instance(X_test.values[90], rf_model.predict_proba)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "f0qugjowJofw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Interpretability with SHAP\n",
        "SHaply Additive exPlanations (SHAP) is a model-agnostic interpretability framework developed to understand the contributions and effects of features on the model outputs. SHAP values help explain how features contribute to model outputs at the local (prediction) level. SHAP does not globally interpret how the model makes its decisions, but rather provides a post-hoc explanation of what features were used and how for different predictions.\n",
        "\n",
        "More on SHAP can be found in the ML interpretability book:\n",
        "https://christophm.github.io/interpretable-ml-book/local-methods.html"
      ],
      "metadata": {
        "id": "LrEx9cwO_RGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.0 Define and train a model (if don't have one already)\n",
        "Example with an XGBoost model."
      ],
      "metadata": {
        "id": "zbnifZ8d_YL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train an XGBoost model\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(objective='binary:logistic')\n",
        "xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Zg0oMXm4_U-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Compute and inspect SHAP values"
      ],
      "metadata": {
        "id": "a5sWUqH-_8hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install shap libraries\n",
        "!pip install shap\n",
        "import shap\n",
        "shap.initjs() # needed for visualisations"
      ],
      "metadata": {
        "id": "vnTWzZ5V_-bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create SHAP explainer, try it with different models (logistic regression, decision tree, xg boost...).\n",
        "\n",
        "> Note: in a random forest, binary classification is sometimes treated as multi-class classification, in which case it is easier to pick a single class to do the SHAP value analysis on. Since this is binary classification, the positive/negative class results are complementary. In the example below, we pick the positive class for the SHAP value analysis.\n"
      ],
      "metadata": {
        "id": "pmDhOqnyAFwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## for a linear model\n",
        "# explainer = shap.LinearExplainer(lr_model, X_train)\n",
        "# shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "## for a tree-based model (decision tree, xgboost, random forest)\n",
        "explainer = shap.TreeExplainer(rf_model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "print(shap_values.shape)\n",
        "\n",
        "# pick positive class for SHAP analysis\n",
        "shap_values_positive_class = shap_values[:, :, 1]\n",
        "print(shap_values_positive_class.shape)"
      ],
      "metadata": {
        "id": "bAtId85QAC17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected value of the model (if multi-class classification, you will get the expected value for each class)."
      ],
      "metadata": {
        "id": "I1rmv8kyJY-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(explainer.expected_value)"
      ],
      "metadata": {
        "id": "UAbzb-CoJQUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Feature importance using SHAP values\n",
        "SHAP values can first be used to understand feature importance by simply looking at the magnitude of the SHAP value. A higher SHAP value indicates a heavier contribution to the model's decision, and therefore more importance."
      ],
      "metadata": {
        "id": "IiSD38nFBDJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we look at a simple barplot of the features, where importance is dictated by the magnitude of the SHAP value."
      ],
      "metadata": {
        "id": "n4CBjBi_BD2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values_positive_class, X_test, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "w2JQlkIzBFvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more detailed analysis, we can look at density SHAP summary plot (same plot but bars are turned into actual values).\n",
        "\n",
        "Each point in the SHAP summary plot represents a **single prediction** made by the model. The colour of each point corresponds to the value of the feature for that prediction. The position on the x-axis shows the SHAP value, which indicates how much each feature contributes to the model's output for that prediction.\n",
        "\n",
        "We can use a SHAP summary plot to identify which features that have a strong influence (large absolute SHAP) and which ones are less impactful (SHAP near 0).\n",
        "The colour of the points helps us understand how the feature's value (high/low) is related to the feature's influence on the model's decision."
      ],
      "metadata": {
        "id": "lVPXfGi2BKOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Which features have the most significant impact on the model's predictions based on the SHAP summary plot? Is that consistent to what you observed before?\n",
        "\n",
        "\n",
        "> What does the distribution of SHAP values tell you about the variability of each feature's impact on the model output? Do features have a consistent impact across different predictions?"
      ],
      "metadata": {
        "id": "FuR5GT-wBKyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values_positive_class, X_test)"
      ],
      "metadata": {
        "id": "k3M7E8CLBIWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Taking a closer look at predictions"
      ],
      "metadata": {
        "id": "mWy4K8Q0BPhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.1 Individual predictions using force plots\n",
        "\n",
        "SHAP force plots help explain the **contribution of individual features** to a **specific prediction**.\n",
        "\n",
        "The base value (expected value) is the starting point of the model's prediction.\n",
        "\n",
        "Features either push the prediction higher (positive SHAP values) or lower (negative SHAP values).\n",
        "The final prediction is the sum of the base value and all SHAP contributions.\n",
        "\n",
        "**Colours** indicate the direction:\n",
        "- Red: feature pushes the prediction higher.\n",
        "- Blue: feature pushes the prediction lower.\n",
        "\n",
        "The length of each feature **arrow** reflects how much it contributes to the decision (how much is pushed away from base value)."
      ],
      "metadata": {
        "id": "YdQQNRGnBSpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look at some individual predictions\n",
        "from IPython.display import display # need to explicitly display force plot in a for loop\n",
        "\n",
        "for i in range(5,10):\n",
        "  y_predicted = y_test.iloc[i]\n",
        "  print('Prediction: ', y_predicted)\n",
        "  shap.initjs()\n",
        "  force_plot = shap.force_plot(explainer.expected_value[1],\n",
        "  shap_values_positive_class[i,:], X_test.iloc[i,:])\n",
        "  display(force_plot)\n",
        "\n"
      ],
      "metadata": {
        "id": "ouJev2daBU04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.2 Visualise multiple predictions by stacking force plots\n",
        "\n",
        "If we pass in all `shap_values` and `X_test`, we get a nice interactive plot that includes all the test points."
      ],
      "metadata": {
        "id": "68N2RfJgBYUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: the samples are ordered by similarity in their features, but you can change the sample ordering in the drop-down menu (both in x and in y)\n",
        "Example: if we pick \"sample order by output value\", we see a nice differentiation between outputs and the role of a key feature - which one?"
      ],
      "metadata": {
        "id": "KssTdvy0BbjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value[1], shap_values_positive_class[:len(X_test),:], X_test.iloc[:len(X_test),:])"
      ],
      "metadata": {
        "id": "tnJ7oenVBdZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
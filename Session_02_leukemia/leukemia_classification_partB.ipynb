{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "94b668c5",
      "metadata": {
        "id": "94b668c5"
      },
      "source": [
        "# **White Blood Cell Classification** (Part B)\n",
        "\n",
        "## Objective\n",
        "In this notebook, you will train a convolutional neural network (ConvNet) to classify white blood cells into two categories:\n",
        "1. Lymphoblasts (indicative of leukemia)\n",
        "2. Normal white blood cells\n",
        "\n",
        "AND test it on an external dataset.\n",
        "\n",
        "The exercise will guide you step-by-step from data loading and preparation to building and training a deep learning model.\n",
        "\n",
        "Types of leukemia: https://www.leukaemiacare.org.uk/types-of-leukaemia/\n",
        "\n",
        "Morphology of leukemia: https://www.sciencedirect.com/science/article/pii/S0185106315000724\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAqBLFGZ6W1R",
      "metadata": {
        "id": "DAqBLFGZ6W1R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc47a56",
      "metadata": {
        "id": "3bc47a56"
      },
      "source": [
        "## 📚 0. Import Libraries (Nothing to do)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc54338d",
      "metadata": {
        "id": "cc54338d"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import tensorflow as tf  # TensorFlow for deep learning\n",
        "import os  # For handling file paths\n",
        "import numpy as np  # For data manipulation\n",
        "import matplotlib.pyplot as plt  # For visualizing the dataset\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For loading and augmenting image data\n",
        "from sklearn.model_selection import train_test_split  # For splitting custom dataset into train and test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418618d9",
      "metadata": {
        "id": "418618d9"
      },
      "source": [
        "## 👓 1. Load the data (Nothing to do)\n",
        "You will use a custom dataset of images that contain lymphoblasts and normal white blood cells. Make sure the dataset is organized in two folders: \"lymphoblast\" and \"normal\", with images placed in their respective folders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5437b1d",
      "metadata": {
        "id": "a5437b1d"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "base_dir = '/content/drive/MyDrive/all_data/'  # Update this to the actual path\n",
        "\n",
        "\n",
        "# Define parameters\n",
        "IMG_HEIGHT, IMG_WIDTH = 128, 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS=10\n",
        "RAN_SEED=17\n",
        "tf.keras.utils.set_random_seed(RAN_SEED)\n",
        "\n",
        "# Use ImageDataGenerator for loading and splitting data\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    validation_split=0.3)  # Split 50% of the data for validationall_data_bright\n",
        "\n",
        "# Creating train and validation datasets\n",
        "train_data = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=RAN_SEED\n",
        ")\n",
        "\n",
        "validation_data = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    seed=RAN_SEED\n",
        ")\n",
        "\n",
        "print(\"Training and validation datasets prepared successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e71707",
      "metadata": {
        "id": "11e71707"
      },
      "source": [
        "###  1.2 : Visualize the Dataset (Part A)\n",
        "\n",
        " Display 10 images from the training dataset to understand the data better. Use the function bellow or build your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b676ad7",
      "metadata": {
        "id": "4b676ad7"
      },
      "outputs": [],
      "source": [
        "# Function to display a few images from the dataset\n",
        "def visualize_dataset(dataset, num_images=5):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, (image, label) in enumerate(dataset):\n",
        "        if i >= num_images:\n",
        "            break\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(image[0])  # Image comes as a batch, so we take the first image\n",
        "        plt.title(\"Lymphoblast\" if label[0] == 1 else \"Normal\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Displaying a few images from the training dataset\n",
        "visualize_dataset(train_data, num_images=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06c927d3",
      "metadata": {
        "id": "06c927d3"
      },
      "source": [
        "## 🧱 3. Build a simple ConvNet (Part 1)\n",
        "\n",
        "Here you will build a simple convolutional neural network (CNN) to classify lymphoblasts and normal white blood cells.\n",
        "\n",
        " **Add in the missing layers**. To prevent overfitting, add also 0.2 dropout rate during training where specified. Hint: check out https://keras.io/api/layers/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa0f1a70",
      "metadata": {
        "id": "fa0f1a70"
      },
      "outputs": [],
      "source": [
        "# Build a simple CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    #Add a pooling layer here. You need to add \",\" after each layer\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    #Add a pooling layer here. You need to add \",\" after each layer\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    #Add a Convolutional layer here with 128 filters. You need to add \",\" after each layer\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "     #Add a dropout layer here with a rate of 0.2. [In sequential mode, you need to add \",\" after each layer]\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How many parameters does your model have?** Hint: use the keras.model API to summarize your model: https://keras.io/api/models/"
      ],
      "metadata": {
        "id": "ghNlS-1y8gYO"
      },
      "id": "ghNlS-1y8gYO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zK0GO88Q8kAH"
      },
      "id": "zK0GO88Q8kAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "018a756d",
      "metadata": {
        "id": "018a756d"
      },
      "source": [
        "### 3.1 Train the Model (Part A)\n",
        "\n",
        "We will now train the model using the training and validation datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852c14e0",
      "metadata": {
        "id": "852c14e0"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=NUM_EPOCHS,  # You can increase the number of epochs based on your needs\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e89cab",
      "metadata": {
        "id": "41e89cab"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_data)\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1573b1fc",
      "metadata": {
        "id": "1573b1fc"
      },
      "source": [
        "### 3.1: Evaluate and Visualize Results (Part A)\n",
        "Evaluate your model on the validation data. Hint: call the apropriate method using the keras API : https://keras.io/api/models/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "# Function to plot ROC curves for multiple models on the same plot\n",
        "def plot_roc_curves(models, validation_data_list, labels, title=\"ROC Curve Comparison\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Iterate over each model and its corresponding validation data\n",
        "    for model, validation_data, label in zip(models, validation_data_list, labels):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        # Get true labels and predicted probabilities\n",
        "        for images, labels_batch in validation_data:\n",
        "            y_true.extend(labels_batch)\n",
        "            preds = model.predict(images)\n",
        "            y_pred.extend(preds)\n",
        "\n",
        "            # Break after one full pass (since it's a generator)\n",
        "            if len(y_true) >= validation_data.samples:\n",
        "                break\n",
        "\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "\n",
        "        # Compute False Positive Rate (FPR) and True Positive Rate (TPR)\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot the ROC Curve for the current model\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Plot the random guess line\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random guess line\n",
        "\n",
        "    # Configure plot settings\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curves for the original model and MobileNet model on the same plot\n",
        "plot_roc_curves(\n",
        "    models=[model],\n",
        "    validation_data_list=[validation_data],\n",
        "    labels=[\"Original Model\", \"Original Model with Data Augmentation\"]\n",
        ")"
      ],
      "metadata": {
        "id": "vKs0YGYIqFMr"
      },
      "id": "vKs0YGYIqFMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. External dataset test (OOD)\n",
        "❗ Here you are going to test your trained models on a separate external dataset.  "
      ],
      "metadata": {
        "id": "w-DJjrRLBlq0"
      },
      "id": "w-DJjrRLBlq0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t424NhxrGXGo",
      "metadata": {
        "id": "t424NhxrGXGo"
      },
      "outputs": [],
      "source": [
        "external_dir = '/content/drive/MyDrive/external_test/'  # Update this to the actual path\n",
        "lymphoblast_dir = os.path.join(base_dir, 'lymphoblast')\n",
        "normal_dir = os.path.join(base_dir, 'healthy')\n",
        "\n",
        "# Define parameters\n",
        "IMG_HEIGHT, IMG_WIDTH = 128, 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Use ImageDataGenerator for loading and splitting data\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,  # Normalize pixel values\n",
        "    validation_split=0.9,  # Split 20% of the data for validation\n",
        ")\n",
        "\n",
        "# Creating train and validation datasets\n",
        "test_data = datagen.flow_from_directory(\n",
        "    external_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗ **Evaluate your model on the external dataset**. How does it compare with the performance on the internal validation dataset? (Plot the two ROCs on the same graph)"
      ],
      "metadata": {
        "id": "ZIPk8gcp9oVz"
      },
      "id": "ZIPk8gcp9oVz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6KB0lBFyHA99",
      "metadata": {
        "id": "6KB0lBFyHA99"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curves for the original model and MobileNet model on the same plot\n",
        "## ... add your code here ... ##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗ **Compute the confusion matrix on the external test set**. What do you notice? Use the function bellow or write your own. Check https://scikit-learn.org/dev/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n"
      ],
      "metadata": {
        "id": "zllj3a6d-NPa"
      },
      "id": "zllj3a6d-NPa"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "# Function to compute and plot the confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap(\"Blues\"))\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    # Print values inside the matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Collect true labels and predictions for the ConvNet model\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels_batch in test_data:\n",
        "    y_true.extend(labels_batch)\n",
        "    preds = model.predict(images)\n",
        "    y_pred.extend((preds > 0.5).astype(int).flatten())  # Convert probabilities to binary labels\n",
        "\n",
        "    if len(y_true) >= test_data.samples:\n",
        "        break\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "## add .. your code here ... ###\n"
      ],
      "metadata": {
        "id": "lwZSCnnNtLIJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lwZSCnnNtLIJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa_NNJJ8pL9M"
      },
      "source": [
        "## 🔧 5. Data Augmentation\n",
        "\n",
        "To make our model more robust and reduce overfitting, here you will apply data augmentation techniques.\n",
        "In Part A, we used augmentation layers. Here, instead we are going to use the ImageDataGenerator (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) instead to have access to more augmentations. The model is identical to the prevous one.    \n",
        "\n",
        "❗ **(Optional) Add more augmentations.**. Change the augmentation parameters and see if it affects the model performance.  "
      ],
      "id": "aa_NNJJ8pL9M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple CNN model (identical to the 1st one)\n",
        "model_augment = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.RandomFlip(),\n",
        "\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    #Add a pooling layer here. You need to add \",\" after each layer\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    #Add a pooling layer here. You need to add \",\" after each layer\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    #Add a Convolutional layer here with 128 filters. You need to add \",\" after each layer\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "     #Add a dropout layer here with a rate of 0.2. [In sequential mode, you need to add \",\" after each layer]\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_augment.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "Qf76KXVqpNaN"
      },
      "id": "Qf76KXVqpNaN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=180,  # Randomly rotate images by up to x degrees\n",
        "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,  # Randomly shift images vertically\n",
        "    shear_range=0.2,  # Randomly apply shear transformation\n",
        "    zoom_range=0.2,  # Randomly zoom in\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    brightness_range=[0.8, 1.5],  # Randomly adjust the brightness of images\n",
        "    fill_mode='nearest',  # Fill empty pixels created by transformations\n",
        "    validation_split=0.3  # Maintain 30% for validation\n",
        ")\n",
        "\n",
        "# Creating new train and validation datasets with augmented images including color transformations\n",
        "augmented_train_data = augmented_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=RAN_SEED\n",
        ")\n",
        "\n",
        "augmented_validation_data = augmented_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    seed=RAN_SEED\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "wf4lmrCwOaMQ"
      },
      "id": "wf4lmrCwOaMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2 ❗ **Train the model.** Train this new model using the augmented data from above.  "
      ],
      "metadata": {
        "id": "Ikfkn21YEM0m"
      },
      "id": "Ikfkn21YEM0m"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train this new model with augmented data\n",
        "history_augmented = ## .. add your code here ... ##\n"
      ],
      "metadata": {
        "id": "CrVli97bELkr"
      },
      "id": "CrVli97bELkr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsQTJtLlp4eB"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model_augment.evaluate(augmented_validation_data)\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "acc = history_augmented.history['accuracy']\n",
        "val_acc = history_augmented.history['val_accuracy']\n",
        "loss = history_augmented.history['loss']\n",
        "val_loss = history_augmented.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()\n"
      ],
      "id": "IsQTJtLlp4eB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗**Plot the ROC curves and compute the AUC for each of your trained models on the external dataset**"
      ],
      "metadata": {
        "id": "H9B52ebgGIwH"
      },
      "id": "H9B52ebgGIwH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMven4SzLmvn"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curves for the original model and MobileNet model on the same plot\n",
        " ## ... add your code here ... ###"
      ],
      "id": "lMven4SzLmvn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗**Compute the confusion  matrix on the external test set for the model trained with the augmented dataset**. How many FP/FN do you notice?"
      ],
      "metadata": {
        "id": "zQ4DcbWxGUea"
      },
      "id": "zQ4DcbWxGUea"
    },
    {
      "cell_type": "code",
      "source": [
        "## .. add your code here .. ##"
      ],
      "metadata": {
        "id": "LY1s-Ls2_IPx"
      },
      "id": "LY1s-Ls2_IPx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "FpBORcAkrTaE",
      "metadata": {
        "id": "FpBORcAkrTaE"
      },
      "source": [
        "## ➡️ 4. Transfer Learning (Part A)\n",
        "\n",
        "❗**Train a MobileNet model pretrained on ImageNet**. Train only the FC layers (freezing the convnet layers). Same as Part A  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hlj7hC6SrUwW",
      "metadata": {
        "id": "Hlj7hC6SrUwW"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Input\n",
        "# Load MobileNetV2 pretrained on ImageNet and add custom layers for our binary classification task\n",
        "NUM_EPOCHS=5\n",
        "input_tensor = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "# create the base pre-trained model on ImageNet with a custom input tensor\n",
        "base_model = MobileNet(\n",
        "    input_tensor=input_tensor,\n",
        "    include_top=False,  # We do not want the top (classification) layers from MobileNet\n",
        "    weights='imagenet'  # Load weights pretrained on ImageNet\n",
        ")\n",
        "\n",
        "# Freeze the base model to use it as a feature extractor\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "model_mobilenet = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    ## ... add code here ##\n",
        "    # Add a global spatial average pooling layer\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    # let's add a fully-connected layer\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),  # Add dropout to prevent overfitting\n",
        "    # and a classification layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "\n",
        "#model_mobilenet2 = tensorflow.keras.models.clone_model(model_mobilenet)\n",
        "\n",
        "# Compile the model\n",
        "model_mobilenet.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "#model_mobilenet.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7_OtkBDlrbSJ",
      "metadata": {
        "id": "7_OtkBDlrbSJ"
      },
      "source": [
        "### 4.1 Train the MobileNet Model (Nothing to do)\n",
        "\n",
        "You will now train the model using the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PlopdD4orekB",
      "metadata": {
        "id": "PlopdD4orekB"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history_mobilenet = model_mobilenet.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=NUM_EPOCHS,  # You can increase the number of epochs based on your needs\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uc58vYq1ut_r",
      "metadata": {
        "id": "Uc58vYq1ut_r"
      },
      "source": [
        "### 4.2 Visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hg7zkbFYusLs",
      "metadata": {
        "id": "Hg7zkbFYusLs"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model_mobilenet.evaluate(validation_data)\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "acc = history_mobilenet.history['accuracy']\n",
        "val_acc = history_mobilenet.history['val_accuracy']\n",
        "loss = history_mobilenet.history['loss']\n",
        "val_loss = history_mobilenet.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2VhzF3vJwKGT",
      "metadata": {
        "id": "2VhzF3vJwKGT"
      },
      "source": [
        "### 🔧 4.3 Data Augmentation\n",
        "\n",
        "To make our model more robust and reduce overfitting, here you will apply data augmentation techniques.\n",
        "\n",
        "\n",
        "❗ **Train the MobileNet model on the augmented dataset**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XF7wcaj4wK-O",
      "metadata": {
        "id": "XF7wcaj4wK-O"
      },
      "outputs": [],
      "source": [
        "# Adding data augmentation to the training dataset\n",
        "\n",
        "# Add augmentation layers before the base model\n",
        "model_mobilenet2 = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),  # Add dropout to prevent overfitting\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "# Compile the model\n",
        "model_mobilenet2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "# Train the MobileNet 2 model with augmented data\n",
        "history_augmented = ## .. add your code here .. ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XfPA_8c4wQwP",
      "metadata": {
        "id": "XfPA_8c4wQwP"
      },
      "source": [
        "### 4.4 Visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fEUYKjCqwRvF",
      "metadata": {
        "id": "fEUYKjCqwRvF"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model_mobilenet2.evaluate(validation_data)\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "acc = history_augmented.history['accuracy']\n",
        "val_acc = history_augmented.history['val_accuracy']\n",
        "loss = history_augmented.history['loss']\n",
        "val_loss = history_augmented.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Compare the performance of your models\n",
        "\n",
        "❗**Plot the ROC curves and compute the AUC for each of your trained models**. Use the function bellow or build your own.  "
      ],
      "metadata": {
        "id": "sFPpF0Mb_nGl"
      },
      "id": "sFPpF0Mb_nGl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EGIN8b2gBHpH",
      "metadata": {
        "id": "EGIN8b2gBHpH"
      },
      "outputs": [],
      "source": [
        "## ... add your code here .. ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PrLknYFcG-lF",
      "metadata": {
        "id": "PrLknYFcG-lF"
      },
      "source": [
        "❗**Show the confusion matrices**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "## .. add your code here .. #\n"
      ],
      "metadata": {
        "id": "4rGbdJacpQ0W"
      },
      "id": "4rGbdJacpQ0W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗**Have a look at the predictions of your models on the external test dataset. Show examples of TP, FP, TN and FN**. You can use the function bellow or write your own."
      ],
      "metadata": {
        "id": "wS62La0LFht0"
      },
      "id": "wS62La0LFht0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot examples of True Positives, True Negatives, False Positives, and False Negatives\n",
        "def plot_examples(validation_data, y_true, y_pred, model, num_examples=3):\n",
        "    categories = {\n",
        "        \"True Positives\": (1, 1),\n",
        "        \"True Negatives\": (0, 0),\n",
        "        \"False Positives\": (0, 1),\n",
        "        \"False Negatives\": (1, 0)\n",
        "    }\n",
        "\n",
        "    for category, (true_val, pred_val) in categories.items():\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.suptitle(category)\n",
        "        count = 0\n",
        "\n",
        "        for i, (image_batch, label_batch) in enumerate(validation_data):\n",
        "            preds = model.predict(image_batch)\n",
        "            binary_preds = (preds > 0.5).astype(int).flatten()\n",
        "            #print(preds)\n",
        "            #print(binary_preds)\n",
        "\n",
        "            for img, true, pred in zip(image_batch, label_batch, binary_preds):\n",
        "                if count >= num_examples:\n",
        "                    break\n",
        "                if true == true_val and pred == pred_val:\n",
        "                    plt.subplot(1, num_examples, count + 1)\n",
        "                    plt.imshow(img)\n",
        "                    plt.title(f\"Label: {true}, Pred: {pred}\")\n",
        "                    plt.axis('off')\n",
        "                    count += 1\n",
        "\n",
        "            if count >= num_examples:\n",
        "                break\n",
        "        plt.show()\n",
        "\n",
        "# Visualize examples of each category\n",
        "## .. add your code here .. ##\n"
      ],
      "metadata": {
        "id": "Epnsx7pCpZd_"
      },
      "id": "Epnsx7pCpZd_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗**(Optional) Try fine-tuning the models on the new dataset**. (Train on a few examples for a very few number of epochs)"
      ],
      "metadata": {
        "id": "PGKiQvX-Y4tz"
      },
      "id": "PGKiQvX-Y4tz"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "94b668c5",
      "metadata": {
        "id": "94b668c5"
      },
      "source": [
        "# **MRI Brain Classification**\n",
        "\n",
        "## Objective\n",
        "In this notebook, you will train a convolutional neural network (ConvNet) to classify white blood cells into two categories:\n",
        "1. Scans of brains with tumors (2D Slices)\n",
        "2. Normal brain scans (2D slices)\n",
        "\n",
        "\n",
        "We will use a custom dataset for this classification task. The exercise will guide you step-by-step from data loading and preparation to building and training a deep learning model and visualizing the saliency maps of the models.\n",
        "\n",
        "Dataset source: https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAqBLFGZ6W1R",
      "metadata": {
        "id": "DAqBLFGZ6W1R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc47a56",
      "metadata": {
        "id": "3bc47a56"
      },
      "source": [
        "## üìö 0. Import Libraries (Nothing to do)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc54338d",
      "metadata": {
        "id": "cc54338d"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import tensorflow as tf  # TensorFlow for deep learning\n",
        "import os  # For handling file paths\n",
        "import numpy as np  # For data manipulation\n",
        "import matplotlib.pyplot as plt  # For visualizing the dataset\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For loading and augmenting image data\n",
        "from sklearn.model_selection import train_test_split  # For splitting custom dataset into train and test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418618d9",
      "metadata": {
        "id": "418618d9"
      },
      "source": [
        "## üëì 1. Download the data (Nothing to do)\n",
        "You will use a custom dataset of images that contain lymphoblasts and normal white blood cells. Make sure the dataset is organized in two folders: \"tumour\" and \"normal\", with images placed in their respective folders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5437b1d",
      "metadata": {
        "id": "a5437b1d"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "base_dir = '/content/drive/MyDrive/small_mri/'  # Update this to the actual path\n",
        "\n",
        "# Define parameters\n",
        "IMG_HEIGHT, IMG_WIDTH = 128, 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS=3\n",
        "RAN_SEED=17\n",
        "\n",
        "# Use ImageDataGenerator for loading and splitting data\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,  # Normalize pixel values\n",
        "    validation_split=0.5)  # Split 50% of the data for validation\n",
        "\n",
        "# Creating train and validation datasets\n",
        "train_data = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=RAN_SEED\n",
        ")\n",
        "\n",
        "validation_data = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    seed=RAN_SEED\n",
        ")\n",
        "\n",
        "print(\"Training and validation datasets prepared successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e71707",
      "metadata": {
        "id": "11e71707"
      },
      "source": [
        "###  1.2 : Visualize the Dataset\n",
        "\n",
        "Display 10 images from the training dataset to understand the data better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b676ad7",
      "metadata": {
        "id": "4b676ad7"
      },
      "outputs": [],
      "source": [
        "# Function to display a few images from the dataset\n",
        "def visualize_dataset(dataset, num_images=5):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, (image, label) in enumerate(dataset):\n",
        "        if i >= num_images:\n",
        "            break\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(image[0])  # Image comes as a batch, so we take the first image\n",
        "        plt.title(\"Tumour\" if label[0] == 1 else \"Normal\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Displaying a few images from the training dataset\n",
        "visualize_dataset(train_data, num_images=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FpBORcAkrTaE",
      "metadata": {
        "id": "FpBORcAkrTaE"
      },
      "source": [
        "## ‚û°Ô∏è 2. Transfer Learning\n",
        "\n",
        "In this section, you will use a pretrained MobileNet model, leveraging its features similar to the lymphoblasts vs normal white blood cells task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hlj7hC6SrUwW",
      "metadata": {
        "id": "Hlj7hC6SrUwW"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Input\n",
        "# Load MobileNetV2 pretrained on ImageNet and add custom layers for our binary classification task\n",
        "input_tensor = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "# create the base pre-trained model on ImageNet with a custom input tensor\n",
        "base_model = MobileNet(\n",
        "    input_tensor=input_tensor,\n",
        "    include_top=False,  # We do not want the top (classification) layers from MobileNet\n",
        "    weights='imagenet'  # Load weights pretrained on ImageNet\n",
        ")\n",
        "\n",
        "# Freeze the base model to use it as a feature extractor\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(128, activation='relu')(x)\n",
        "# and a classification layer\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model_mobilenet = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional MobileNet layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model_mobilenet.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "#model_mobilenet.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7_OtkBDlrbSJ",
      "metadata": {
        "id": "7_OtkBDlrbSJ"
      },
      "source": [
        "### 2.1 Train the MobileNet Model (Nothing to do)\n",
        "\n",
        "You will now train the model using the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PlopdD4orekB",
      "metadata": {
        "id": "PlopdD4orekB"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history_mobilenet = model_mobilenet.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=NUM_EPOCHS,  # You can increase the number of epochs based on your needs\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uc58vYq1ut_r",
      "metadata": {
        "id": "Uc58vYq1ut_r"
      },
      "source": [
        "### 2.2 Visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hg7zkbFYusLs",
      "metadata": {
        "id": "Hg7zkbFYusLs"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model_mobilenet.evaluate(validation_data)\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "acc = history_mobilenet.history['accuracy']\n",
        "val_acc = history_mobilenet.history['val_accuracy']\n",
        "loss = history_mobilenet.history['loss']\n",
        "val_loss = history_mobilenet.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Compare the performance of your models\n",
        "\n",
        "**Plot the ROC curves and compute the AUC for your trained model**."
      ],
      "metadata": {
        "id": "sFPpF0Mb_nGl"
      },
      "id": "sFPpF0Mb_nGl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EGIN8b2gBHpH",
      "metadata": {
        "id": "EGIN8b2gBHpH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "# Function to plot ROC curves for multiple models on the same plot\n",
        "def plot_roc_curves(models, validation_data_list, labels, title=\"ROC Curve Comparison\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Iterate over each model and its corresponding validation data\n",
        "    for model, validation_data, label in zip(models, validation_data_list, labels):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        # Get true labels and predicted probabilities\n",
        "        for images, labels_batch in validation_data:\n",
        "            y_true.extend(labels_batch)\n",
        "            preds = model.predict(images)\n",
        "            y_pred.extend(preds)\n",
        "\n",
        "            # Break after one full pass (since it's a generator)\n",
        "            if len(y_true) >= validation_data.samples:\n",
        "                break\n",
        "\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "\n",
        "        # Compute False Positive Rate (FPR) and True Positive Rate (TPR)\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot the ROC Curve for the current model\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Plot the random guess line\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random guess line\n",
        "\n",
        "    # Configure plot settings\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot the ROC curve\n",
        "plot_roc_curves(\n",
        "    models=[model_mobilenet],\n",
        "    validation_data_list=[validation_data],\n",
        "    labels=[\"MobileNet Model\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÜ 3. Class Activation Maps\n",
        "\n",
        "In this section, you need to use the GradCAM approach (https://arxiv.org/abs/1610.02391) to visualize the activation maps on some test images.\n",
        "Below, there is a function which implements the GradCAM algorithm. The function needs the image, the trained model and *the name of the last convolutional layer in the model*.  "
      ],
      "metadata": {
        "id": "7yq3rKX-wryl"
      },
      "id": "7yq3rKX-wryl"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def get_img_array(img, size):\n",
        "\n",
        "    #array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    img=np.resize(img,(size,size,3)) #.astype(np.float32)\n",
        "    array = np.expand_dims(img, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "    grad_model = Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        #class_channel = preds[:, pred_index]\n",
        "        class_channel = preds #[:, 0]\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "otV6zcwisdB4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "otV6zcwisdB4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 GradCAM on our model\n",
        "\n",
        "We're going to select 16 images from the test set and apply the GradCAM to each of them. To each image an extra dimension is added so that their shape is (1,128,128,3). Why do we need that that?\n",
        "\n",
        "‚ùó You need to call the make_grad_heatmap() function with the right arguments. Hint: look at the model summary to get the names of all the layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "F5_uLDVKdZHV"
      },
      "id": "F5_uLDVKdZHV"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "img_size=128\n",
        "\n",
        "\n",
        "image_examples=[]\n",
        "labels_examples=[]\n",
        "cam_examples=[]\n",
        "\n",
        "\n",
        "images, labels=validation_data[1]\n",
        "  # Prepare image\n",
        "\n",
        "for layer in model_mobilenet.layers:\n",
        "    print(layer.name)\n",
        "\n",
        "  # Iterate over the images and labels in the current batch\n",
        "for image, label in zip(images, labels):\n",
        "  image_examples.append(image)\n",
        "  labels_examples.append(label)\n",
        "\n",
        "  image_converted = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize image\n",
        "  image_resized = tf.image.resize(image_converted, (IMG_HEIGHT, IMG_WIDTH))\n",
        "  img_array=tf.expand_dims(image_resized,axis=0)\n",
        "\n",
        "  # Make model\n",
        "  model = model_mobilenet\n",
        "\n",
        "  # Print what the top predicted class is\n",
        "  preds = model.predict(img_array)\n",
        "  print(\"Label: \", label, \"Predicted:\", preds[0]) #decode_predictions(preds, top=1)[0])\n",
        "\n",
        "\n",
        "  heatmap = ### write your code here ... ###\n",
        "\n",
        "  cam_examples.append(heatmap)\n",
        "\n",
        "# Display last heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jqrExt_zsmhV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jqrExt_zsmhV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the CAMs is quite small.\n",
        "You need to rescale them to the original image size and then superimpose them to the corresponding image. The *display_gradcam* function below creates a transparent heatmap corresponding to the CAM and adds it to the image.\n",
        "\n",
        "‚ùóYou need to modify this function so that the size of the heatmap equals the size of the image. Easy.\n",
        "‚ùóEqually you need to create a blended image (weighted sum) of the original image and the CAM (use the alpha value to control  transparency)"
      ],
      "metadata": {
        "id": "DnPT9sGwqS8u"
      },
      "id": "DnPT9sGwqS8u"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import cm\n",
        "\n",
        "def display_gradcam_with_colorbar(img, heatmap, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Superimposes the Grad-CAM heatmap onto the original image and adds a colorbar.\n",
        "\n",
        "    Args:\n",
        "    - img: Original image as a NumPy array (H, W, C).\n",
        "    - heatmap: Grad-CAM heatmap as a NumPy array (H, W).\n",
        "    - alpha: Intensity factor for the heatmap overlay.\n",
        "\n",
        "    Returns:\n",
        "    - superimposed_img: Image with heatmap overlay as a PIL Image.\n",
        "    \"\"\"\n",
        "    # Ensure the image is a NumPy array and normalize it to 0-255\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = (img - img.min()) / (img.max() - img.min())  # Normalize to 0-1\n",
        "    img = np.uint8(img * 255)  # Scale to 0-255\n",
        "\n",
        "    # Rescale heatmap to a range of 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Use jet colormap to colorize the heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "    jet_colors = jet(np.arange(256))[:, :3]  # RGB values for the colormap\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB heatmap\n",
        "    jet_heatmap = tf.keras.utils.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = ### write your code here...\n",
        "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on the original image\n",
        "    superimposed_img = ### write your code here ... ###\n",
        "    superimposed_img = tf.keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "    return superimposed_img\n",
        "\n",
        "# Visualize Grad-CAMs for multiple examples\n",
        "def visualize_gradcams_with_colorbars(image_examples, cam_examples, labels_examples, class_names, rows=4, cols=4):\n",
        "    \"\"\"\n",
        "    Visualizes Grad-CAM heatmaps overlaid on images in a grid format with colorbars.\n",
        "\n",
        "    Args:\n",
        "    - image_examples: List or array of input images.\n",
        "    - cam_examples: List or array of heatmaps corresponding to the images.\n",
        "    - labels_examples: List or array of true labels for the images.\n",
        "    - class_names: List of class names corresponding to the labels.\n",
        "    - rows: Number of rows in the grid.\n",
        "    - cols: Number of columns in the grid.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(20, 20))\n",
        "\n",
        "    for j, ax in enumerate(axes.flat):\n",
        "        if j >= len(image_examples):\n",
        "            break\n",
        "\n",
        "        # Display the Grad-CAM superimposed image\n",
        "        img = display_gradcam_with_colorbar(image_examples[j], cam_examples[j])\n",
        "\n",
        "        # Show the image without calling plt.show() again\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(class_names[int(labels_examples[j])])\n",
        "        ax.axis('off')\n",
        "        cbar = fig.colorbar(cm.ScalarMappable(cmap=\"jet\"), ax=ax, orientation=\"vertical\")\n",
        "        cbar.set_label(\"Activation Intensity\", rotation=270, labelpad=15)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming `image_examples`, `cam_examples`, and `labels_examples` are predefined\n",
        "# and `class_names` contains the class labels (e.g., ['normal', 'tumor'])\n",
        "visualize_gradcams_with_colorbars(image_examples, cam_examples, labels_examples, class_names=['normal', 'tumor'])\n"
      ],
      "metadata": {
        "id": "Dd3ha9QTOJeT"
      },
      "id": "Dd3ha9QTOJeT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî• 4. Pixel-wise saliency maps\n",
        "\n",
        "‚ùó You need to complete the pixel-wise saliency map computation here.   \n",
        "Have look at this paper : https://arxiv.org/abs/1312.6034\n",
        "which does a deep dive into the algorithm. Equally, check out the keras visualisation kit (https://github.com/keisen/tf-keras-vis)."
      ],
      "metadata": {
        "id": "7PoAzyvmb8F-"
      },
      "id": "7PoAzyvmb8F-"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_saliency_map(model, img_array, class_index=None):\n",
        "    \"\"\"\n",
        "    Computes a pixel-wise saliency map using the gradients of the model's output with respect to the input image.\n",
        "\n",
        "    Args:\n",
        "    - model: Trained model.\n",
        "    - img_array: Preprocessed input image as a NumPy array (1, H, W, C).\n",
        "    - class_index: Index of the class to compute the saliency for (optional).\n",
        "\n",
        "    Returns:\n",
        "    - saliency_map: Saliency map as a NumPy array (H, W).\n",
        "    \"\"\"\n",
        "    # Ensure the input image is wrapped in a batch\n",
        "    if len(img_array.shape) == 3:\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Use GradientTape to compute gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Watch the input image\n",
        "        tape.watch(img_array)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(img_array)\n",
        "\n",
        "        # Target the specific class or maximum score\n",
        "        #if class_index is None:\n",
        "        #    class_index = tf.argmax(predictions[0])\n",
        "\n",
        "        target_score = predictions\n",
        "        #target_score = predictions[:, class_index]\n",
        "\n",
        "    # Compute the gradient of the target score with respect to the input image\n",
        "    gradients = tape.gradient(target_score, img_array)\n",
        "\n",
        "    # Take the absolute value of the gradients and reduce to a single channel\n",
        "    saliency_map = ## ..write your code here ..\n",
        "\n",
        "    # Normalize the saliency map to the range [0, 1]\n",
        "    saliency_map = (saliency_map - tf.reduce_min(saliency_map)) / (tf.reduce_max(saliency_map) - tf.reduce_min(saliency_map))\n",
        "\n",
        "    return saliency_map.numpy()"
      ],
      "metadata": {
        "id": "8kW8M3e3THI0"
      },
      "id": "8kW8M3e3THI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùóDisplay the saliency maps on top of the images in a similar way you did for the GradCAMs."
      ],
      "metadata": {
        "id": "Rw9u4petlsN9"
      },
      "id": "Rw9u4petlsN9"
    },
    {
      "cell_type": "code",
      "source": [
        "image_examples=[]\n",
        "labels_examples=[]\n",
        "sal_examples=[]\n",
        "\n",
        "\n",
        "images, labels=validation_data[2]\n",
        "\n",
        "\n",
        "for image, label in zip(images, labels):\n",
        "  image_examples.append(image)\n",
        "  labels_examples.append(label)\n",
        "\n",
        "  image_converted = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize image\n",
        "  image_resized = tf.image.resize(image_converted, (IMG_HEIGHT, IMG_WIDTH))\n",
        "  img_array=tf.expand_dims(image_resized,axis=0)\n",
        "\n",
        "\n",
        "  # Remove last layer's softmax\n",
        "  #model.layers[-1].activation = None\n",
        "\n",
        "  # Print what the top predicted class is\n",
        "  preds = model.predict(img_array)\n",
        "  print(\"Label: \", label, \"Predicted:\", preds[0])#decode_predictions(preds, top=1)[0])\n",
        "\n",
        "\n",
        "  # Generate saliency maps\n",
        "  ## write your code here .....\n",
        "\n",
        "# Display images\n",
        "### write your code here ...."
      ],
      "metadata": {
        "id": "FLGLlTjvTWRA"
      },
      "id": "FLGLlTjvTWRA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùóWhat do you notice?\n",
        "\n",
        "‚ùóTry out SmoothGrad or Integrated Gradients\n",
        "\n",
        "‚ùóTry training your model with data augmentation. Do the saliency maps change?"
      ],
      "metadata": {
        "id": "zeaYgiu4l_eB"
      },
      "id": "zeaYgiu4l_eB"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem :**\n",
        "\n",
        "Prediction of type 2 diabetes mellitus (T2DM) at an early stage can lead to improved treatment and increased quality of life. Diagnosis of diabetes is considered a challenging problem at an early stage. A single parameter is not very effective to accurately diagnose diabetes and may be misleading in the decision making process. There is a need to combine different parameters to effectively predict diabetes at an early stage. You need to develop a machine learning model that can predict whether people have diabetes based on a subset of their clinical data.\n",
        "\n",
        "\n",
        "Read more:\n",
        "\n",
        "https://www.nature.com/articles/s41598-020-61123-x\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S2352914819300176 (dataset description)\n",
        "\n",
        "\n",
        "\n",
        "### **Dataset :**\n",
        "\n",
        "The dataset is part of the large dataset held at the National Institutes of Diabetes-Digestive-Kidney Diseases (NIH). The target variable is specified as \"Outcome\"; 1 indicates positive diabetes test result, 0 indicates negative.\n",
        "\n",
        "### **Variables :**\n",
        "* Pregnancies    : Number of pregnancies\n",
        "* Glucose        : 2-hour plasma glucose concentration in the oral glucose tolerance test\n",
        "* Blood Pressure : Blood Pressure (Smallness) (mm Hg)\n",
        "* SkinThickness  : Skin Thickness\n",
        "* Insulin        : 2-hour serum insulin (mu U/ml)\n",
        "* Diabetes Pedigree Function : Function (2 hour plasma glucose concentration in oral glucose tolerance test)\n",
        "* BMI            : Body mass index\n",
        "* Age            : Age (years)\n",
        "* Outcome        : Have the disease(1) or not (0)"
      ],
      "metadata": {
        "id": "g8ppk0s6SO2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Libraries (Nothing to do)"
      ],
      "metadata": {
        "id": "g9A2ZxHTSO2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "# !pip install missingno\n",
        "import missingno as msno\n",
        "from datetime import date\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:46.575984Z",
          "iopub.execute_input": "2022-01-17T13:22:46.576281Z",
          "iopub.status.idle": "2022-01-17T13:22:48.037217Z",
          "shell.execute_reply.started": "2022-01-17T13:22:46.576244Z",
          "shell.execute_reply": "2022-01-17T13:22:48.0363Z"
        },
        "trusted": true,
        "id": "Hcvt4v9wSO2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some adjustments\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "pd.set_option('display.width', 500)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:48.039167Z",
          "iopub.execute_input": "2022-01-17T13:22:48.040148Z",
          "iopub.status.idle": "2022-01-17T13:22:48.047352Z",
          "shell.execute_reply.started": "2022-01-17T13:22:48.0401Z",
          "shell.execute_reply": "2022-01-17T13:22:48.046217Z"
        },
        "trusted": true,
        "id": "k4CcTDENSO2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé 1. Reading the Dataset (Nothing to do)\n",
        "\n",
        "You can alternatively use colab's [import file feature](https://colab.research.google.com/notebooks/io.ipynb)"
      ],
      "metadata": {
        "id": "9kbpo0SuSO2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "df_ = pd.read_csv('diabetes.csv')\n",
        "df = df_.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:48.048869Z",
          "iopub.execute_input": "2022-01-17T13:22:48.049258Z",
          "iopub.status.idle": "2022-01-17T13:22:48.075845Z",
          "shell.execute_reply.started": "2022-01-17T13:22:48.049216Z",
          "shell.execute_reply": "2022-01-17T13:22:48.075222Z"
        },
        "trusted": true,
        "id": "eVrpAX_KSO2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# auxiliary functions\n",
        "def check_df(dataframe):\n",
        "    print(\"##################### Shape #####################\")\n",
        "    print(dataframe.shape)\n",
        "    print(\"##################### Types #####################\")\n",
        "    print(dataframe.dtypes)\n",
        "    print(\"##################### Head #####################\")\n",
        "    print(dataframe.head(3))\n",
        "    print(\"##################### Tail #####################\")\n",
        "    print(dataframe.tail(3))\n",
        "    print(\"##################### NA #####################\")\n",
        "    print(dataframe.isnull().sum())\n",
        "    print(\"##################### Quantiles #####################\")\n",
        "    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n",
        "\n",
        "\n",
        "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ------\n",
        "        dataframe: dataframe\n",
        "\n",
        "        cat_th: int, optional\n",
        "\n",
        "        car_th: int, optinal\n",
        "\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "        cat_cols: list\n",
        "                Categorical features\n",
        "        num_cols: list\n",
        "                Numerical features\n",
        "        cat_but_car: list\n",
        "               Categorical view cardinal variable list\n",
        "\n",
        "    Examples\n",
        "    ------\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"iris\")\n",
        "        print(grab_col_names(df))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # cat_cols, cat_but_car\n",
        "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
        "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
        "                   dataframe[col].dtypes != \"O\"]\n",
        "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
        "                   dataframe[col].dtypes == \"O\"]\n",
        "    cat_cols = cat_cols + num_but_cat\n",
        "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
        "\n",
        "    # num_cols\n",
        "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
        "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
        "\n",
        "    print(f\"Observations: {dataframe.shape[0]}\")\n",
        "    print(f\"Variables: {dataframe.shape[1]}\")\n",
        "    print(f'cat_cols: {len(cat_cols)}')\n",
        "    print(f'num_cols: {len(num_cols)}')\n",
        "    print(f'cat_but_car: {len(cat_but_car)}')\n",
        "    print(f'num_but_cat: {len(num_but_cat)}')\n",
        "    return cat_cols, num_cols, cat_but_car\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def missing_values_table(dataframe, na_name=False):\n",
        "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
        "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
        "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
        "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
        "    print(missing_df, end=\"\\n\")\n",
        "    if na_name:\n",
        "        return na_columns\n",
        "\n",
        "def missing_vs_target(dataframe, target, na_columns):\n",
        "    temp_df = dataframe.copy()\n",
        "\n",
        "    for col in na_columns:\n",
        "        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n",
        "\n",
        "    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n",
        "\n",
        "    for col in na_flags:\n",
        "        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n",
        "                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n",
        "\n",
        "df.columns = [col.upper() for col in df.columns]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:48.079426Z",
          "iopub.execute_input": "2022-01-17T13:22:48.079901Z",
          "iopub.status.idle": "2022-01-17T13:22:48.109537Z",
          "shell.execute_reply.started": "2022-01-17T13:22:48.079869Z",
          "shell.execute_reply": "2022-01-17T13:22:48.108907Z"
        },
        "trusted": true,
        "id": "bbNwradwSO2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé 2. Data preparation"
      ],
      "metadata": {
        "id": "ZeqIE8Q3SO2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_df(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:48.111101Z",
          "iopub.execute_input": "2022-01-17T13:22:48.111333Z",
          "iopub.status.idle": "2022-01-17T13:22:48.152068Z",
          "shell.execute_reply.started": "2022-01-17T13:22:48.111305Z",
          "shell.execute_reply": "2022-01-17T13:22:48.150841Z"
        },
        "trusted": true,
        "id": "7n0oOMxASO2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols, num_cols, cat_but_car = grab_col_names(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:48.153671Z",
          "iopub.execute_input": "2022-01-17T13:22:48.15399Z",
          "iopub.status.idle": "2022-01-17T13:22:48.165323Z",
          "shell.execute_reply.started": "2022-01-17T13:22:48.153942Z",
          "shell.execute_reply": "2022-01-17T13:22:48.164244Z"
        },
        "trusted": true,
        "id": "KFZr_9NlSO2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1 Data exploration\n",
        "\n",
        "‚ùó**Plot the label distribution. What is the ratio of positive cases?**\n",
        "(Hint: use the function cat_summary whihc uses seaborns  countplot: https://seaborn.pydata.org/generated/seaborn.countplot.html\n",
        "\n",
        "Alternatively you can use the histplot method directly: https://seaborn.pydata.org/generated/seaborn.histplot.html."
      ],
      "metadata": {
        "id": "ONoYXiCexHz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_summary(dataframe, col_name, plot=False):\n",
        "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
        "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
        "    print(\"##########################################\")\n",
        "    if plot:\n",
        "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "cat_summary(df, \"OUTCOME\",plot=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:48.166899Z",
          "iopub.execute_input": "2022-01-17T13:22:48.16721Z",
          "iopub.status.idle": "2022-01-17T13:22:48.366434Z",
          "shell.execute_reply.started": "2022-01-17T13:22:48.16718Z",
          "shell.execute_reply": "2022-01-17T13:22:48.365876Z"
        },
        "trusted": true,
        "id": "_6VfQ47BSO2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùó**Summarize the features. Plot the distriobution of each feature. Do you notice anything weird?**\n",
        "\n",
        "(Hint: use the histplot seaborn method: https://seaborn.pydata.org/generated/seaborn.histplot.html)\n",
        "\n",
        "Tryout the pairplot method from seaborn also!"
      ],
      "metadata": {
        "id": "a8yUgn4qx1yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def num_summary(dataframe, numerical_col, plot= False):\n",
        "    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
        "    print(dataframe[numerical_col].describe(quantiles).T)\n",
        "\n",
        "    if plot:\n",
        "        dataframe[numerical_col].hist(bins=20)\n",
        "        plt.xlabel(numerical_col)\n",
        "        plt.title(numerical_col)\n",
        "        plt.show()\n",
        "\n",
        "#for col in num_cols:\n",
        "#    num_summary(df, col,plot=True)\n",
        "\n",
        "#sns.pairplot(df, hue='OUTCOME')\n",
        "\n",
        "for col in df.columns:\n",
        "\n",
        "  sns.histplot(data=df, x=col)\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:48.367325Z",
          "iopub.execute_input": "2022-01-17T13:22:48.367717Z",
          "iopub.status.idle": "2022-01-17T13:22:50.302175Z",
          "shell.execute_reply.started": "2022-01-17T13:22:48.367684Z",
          "shell.execute_reply": "2022-01-17T13:22:50.301535Z"
        },
        "trusted": true,
        "id": "C_IFMJMaSO20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XT9In35_ybYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùó**Are patients diagnosed with diabetes older or younger than the rest?** (You can use the function below. Or you  write your own script directly)"
      ],
      "metadata": {
        "id": "baHiQ43syiXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_summary_with_num(dataframe, target, numerical_col):\n",
        "    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\n",
        "\n",
        "target_summary_with_num(df,\"OUTCOME\",\"AGE\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:50.303183Z",
          "iopub.execute_input": "2022-01-17T13:22:50.303763Z",
          "iopub.status.idle": "2022-01-17T13:22:50.315013Z",
          "shell.execute_reply.started": "2022-01-17T13:22:50.303726Z",
          "shell.execute_reply": "2022-01-17T13:22:50.314368Z"
        },
        "trusted": true,
        "id": "gvYKULBqSO20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùó**Plot the correlation matrix between different features (outcome included). Which feature is highest correlated with the target?** (Hint: Use the correlation function in the dataframe type. You can plot it using seaborn heatmap)"
      ],
      "metadata": {
        "id": "KxqQQ1-fzSwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize=[7, 5])\n",
        "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap=\"YlGnBu\")\n",
        "ax.set_title(\"Correlation Matrix\", fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:50.317754Z",
          "iopub.execute_input": "2022-01-17T13:22:50.318406Z",
          "iopub.status.idle": "2022-01-17T13:22:50.983677Z",
          "shell.execute_reply.started": "2022-01-17T13:22:50.318368Z",
          "shell.execute_reply": "2022-01-17T13:22:50.983031Z"
        },
        "trusted": true,
        "id": "sIBkMUCCSO20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé 2.2 Missing Value Analysis\n",
        "\n",
        "At a first glance there are no missing values (NA values).\n",
        "\n"
      ],
      "metadata": {
        "id": "KmDf2DPDSO21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:50.984981Z",
          "iopub.execute_input": "2022-01-17T13:22:50.985636Z",
          "iopub.status.idle": "2022-01-17T13:22:50.994Z",
          "shell.execute_reply.started": "2022-01-17T13:22:50.985604Z",
          "shell.execute_reply": "2022-01-17T13:22:50.993053Z"
        },
        "trusted": true,
        "id": "kYsUTP37SO21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, if you look at BLOODPRESSURE (or BMI or INSULIN for example) you can notice some values which do not make any sense.  "
      ],
      "metadata": {
        "id": "Z5Az4PKstpju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df==0).sum()"
      ],
      "metadata": {
        "id": "klfrHnZLtqq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's replace those values with NaN"
      ],
      "metadata": {
        "id": "NuK7uo1CYKsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"GLUCOSE\",\"BLOODPRESSURE\",\"SKINTHICKNESS\",\"INSULIN\",\"BMI\"]]= df[[\"GLUCOSE\",\"BLOODPRESSURE\",\"SKINTHICKNESS\",\"INSULIN\",\"BMI\"]].replace(0,np.NaN)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:50.995262Z",
          "iopub.execute_input": "2022-01-17T13:22:50.995633Z",
          "iopub.status.idle": "2022-01-17T13:22:51.008171Z",
          "shell.execute_reply.started": "2022-01-17T13:22:50.995605Z",
          "shell.execute_reply": "2022-01-17T13:22:51.007029Z"
        },
        "trusted": true,
        "id": "7jZTebAMSO22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "na_cols = missing_values_table(df, True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:51.010095Z",
          "iopub.execute_input": "2022-01-17T13:22:51.010996Z",
          "iopub.status.idle": "2022-01-17T13:22:51.025685Z",
          "shell.execute_reply.started": "2022-01-17T13:22:51.010933Z",
          "shell.execute_reply": "2022-01-17T13:22:51.02509Z"
        },
        "trusted": true,
        "id": "LgN5inG4SO22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùó**How would you handle missing data?** Hint: you can use the function bellow.\n",
        "Alternatively, use the fillna method: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html.\n",
        "\n",
        "Some examples:\n",
        "https://towardsdatascience.com/8-methods-for-handling-missing-values-with-python-pandas-842544cdf891\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u-6d0Jb0Ymff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def median_target(variable):\n",
        "    temp = df[df[variable].notna()]\n",
        "    temp = temp[[variable, 'OUTCOME']].groupby(['OUTCOME'])[[variable]].median().reset_index()\n",
        "    return temp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:51.740603Z",
          "iopub.execute_input": "2022-01-17T13:22:51.740854Z",
          "iopub.status.idle": "2022-01-17T13:22:51.746751Z",
          "shell.execute_reply.started": "2022-01-17T13:22:51.740823Z",
          "shell.execute_reply": "2022-01-17T13:22:51.745904Z"
        },
        "trusted": true,
        "id": "e8HAmMdMSO23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "columns = df.columns\n",
        "columns = columns.drop(\"OUTCOME\")\n",
        "\n",
        "for col in columns:\n",
        "    df.loc[(df['OUTCOME'] == 0) & (df[col].isna()), col] = median_target(col)[col][0]\n",
        "    df.loc[(df['OUTCOME'] == 1) & (df[col].isna()), col] = median_target(col)[col][1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:51.748476Z",
          "iopub.execute_input": "2022-01-17T13:22:51.749086Z",
          "iopub.status.idle": "2022-01-17T13:22:51.837317Z",
          "shell.execute_reply.started": "2022-01-17T13:22:51.749041Z",
          "shell.execute_reply": "2022-01-17T13:22:51.836165Z"
        },
        "trusted": true,
        "id": "nwS6-EgtSO23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols, num_cols, cat_but_car = grab_col_names(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:52.13508Z",
          "iopub.execute_input": "2022-01-17T13:22:52.13608Z",
          "iopub.status.idle": "2022-01-17T13:22:52.146532Z",
          "shell.execute_reply.started": "2022-01-17T13:22:52.136041Z",
          "shell.execute_reply": "2022-01-17T13:22:52.145549Z"
        },
        "trusted": true,
        "id": "IEqeZFVgSO25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 One-Hot Encoding"
      ],
      "metadata": {
        "id": "rbcnYK3ESO25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df[cat_cols + num_cols], drop_first=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:52.14788Z",
          "iopub.execute_input": "2022-01-17T13:22:52.148259Z",
          "iopub.status.idle": "2022-01-17T13:22:52.164988Z",
          "shell.execute_reply.started": "2022-01-17T13:22:52.148221Z",
          "shell.execute_reply": "2022-01-17T13:22:52.16406Z"
        },
        "trusted": true,
        "id": "NEqFG7uRSO25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Feature Standarzation\n",
        "\n",
        "‚ùó**Why do we need feature standardization?** (Hint: to implement use a Scaler object from python)"
      ],
      "metadata": {
        "id": "SjsWnTjxSO25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization for numerical cols\n",
        "rs = StandardScaler()\n",
        "df[num_cols] = rs.fit_transform(df[num_cols])\n",
        "df.head()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:52.166848Z",
          "iopub.execute_input": "2022-01-17T13:22:52.167122Z",
          "iopub.status.idle": "2022-01-17T13:22:52.197507Z",
          "shell.execute_reply.started": "2022-01-17T13:22:52.167087Z",
          "shell.execute_reply": "2022-01-17T13:22:52.196584Z"
        },
        "trusted": true,
        "id": "c0JPu08pSO25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé 3. Model training and evalation"
      ],
      "metadata": {
        "id": "Yu2oTQ4uSO26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###3.1 Simple model\n",
        "\n",
        "‚ùó**Train a ML model!** Data is already split into train and test. First, try out a [logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). What accuracy do you get?      "
      ],
      "metadata": {
        "id": "_isjxhM7P9Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"OUTCOME\"]\n",
        "X = df.drop([\"OUTCOME\"], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "rf_model = LogisticRegression(random_state=46).fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:52.199068Z",
          "iopub.execute_input": "2022-01-17T13:22:52.199326Z",
          "iopub.status.idle": "2022-01-17T13:22:52.503211Z",
          "shell.execute_reply.started": "2022-01-17T13:22:52.199294Z",
          "shell.execute_reply": "2022-01-17T13:22:52.502129Z"
        },
        "trusted": true,
        "id": "FUEMCXmDSO26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2 Evaluation metrics\n",
        "\n",
        "‚ùó**Plot the results. What other metrics than accuracy can you think of?** Hint: [ROC](https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python). What about precision-recall? Confusion m...?"
      ],
      "metadata": {
        "id": "9l5Cb1ZlS1US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "probs = rf_model.predict_proba(X_test)\n",
        "y_pred_proba = probs[:,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"Logistic Regression, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test,  y_pred_proba)\n",
        "ap=metrics.average_precision_score(y_test, y_pred_proba)\n",
        "plt.plot(recall,precision,label=\"Logistic Regression, ap=\"+str(ap))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cf_matrix, annot=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "DWW8piicTa88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Robust evaluation**\n",
        "\n",
        "‚ùóEvaluate your model's performance on multiple train-test splits. What is the average performance of your model? Try out different metrics.  \n",
        "\n",
        "Hint: [Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html). You can either implement it on your own (for loop) or use the functions available in sklearn."
      ],
      "metadata": {
        "id": "HZSk4wAtUcLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "alt_cv=ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
        "scores = cross_val_score(LogisticRegression(), X, y, cv=alt_cv, scoring='roc_auc')\n",
        "print(scores.mean(), scores.std())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-17T13:22:52.504674Z",
          "iopub.execute_input": "2022-01-17T13:22:52.504899Z",
          "iopub.status.idle": "2022-01-17T13:22:52.912797Z",
          "shell.execute_reply.started": "2022-01-17T13:22:52.504871Z",
          "shell.execute_reply": "2022-01-17T13:22:52.911749Z"
        },
        "trusted": true,
        "id": "r4K8LikDSO26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3 Compare different types of classifiers\n",
        "\n",
        "‚ùóTry an SVM or a Random Forest. Hint: Check out the [sklearn](https://scikit-learn.org/stable/supervised_learning.html) classifiers."
      ],
      "metadata": {
        "id": "dm5ysmCrzsnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"OUTCOME\"]\n",
        "X = df.drop([\"OUTCOME\"], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)\n",
        "\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "probs = rf_model.predict_proba(X_test)\n",
        "y_pred_proba = probs[:,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"Random Forrest, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test,  y_pred_proba)\n",
        "ap=metrics.average_precision_score(y_test, y_pred_proba)\n",
        "plt.plot(recall,precision,label=\"Random Forrest, ap=\"+str(ap))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cf_matrix, annot=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "RQaqA7oczwmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Discussion :"
      ],
      "metadata": {
        "id": "rWa6bvSzSO26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùó **What is the problem you want to solve? Why is it important?**\n",
        "\n",
        "‚ùó **Are there any challenges with the data?**\n",
        "\n",
        "‚ùó**What are the challenges of deploying this a model in a clinical setting? Would it work?**"
      ],
      "metadata": {
        "id": "cqjeMf_TSO27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "hSZoEnKUSO27"
      }
    }
  ]
}